{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25e5ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a72c4300",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "## langsmith ttacking\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACKING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_PROJECT']= os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb135c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Data ingestion\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b57d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader  = WebBaseLoader(\"https://docs.smith.langchain.com/tutorials/Administrators/manage_spend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4eb7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs =loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88086668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/tutorials/Administrators/manage_spend', 'title': 'ü¶úÔ∏èüõ†Ô∏è LangSmith', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could not find what you were looking for.Head back to our main docs page or use the search bar to find the page you need.CommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f380aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPlit data\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4868d72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kami5\\AppData\\Local\\Temp\\ipykernel_13668\\89269069.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98ff1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5ecc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppPage Not FoundWe could not find what you were looking for.Head back to our main docs page or use the search bar to find the page you need.CommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query to a db\n",
    "query=\"LangSmith has two usage limits: total traces and extended\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07335abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, trust_remote_code=True )\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56cbb62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down Generative AI. It‚Äôs a really fascinating and rapidly evolving field, but here‚Äôs a breakdown of what it is, how it works, and why it's so significant:\n",
      "\n",
      "**1. What is Generative AI?**\n",
      "\n",
      "Generative AI refers to a type of artificial intelligence that‚Äôs designed to **create new content** ‚Äì text, images, audio, video, code, and more ‚Äì rather than just analyze or classify existing data.  It learns from massive amounts of data and then uses that knowledge to generate something original.\n",
      "\n",
      "**2. Key Technologies Behind Generative AI**\n",
      "\n",
      "There are several key technologies that enable generative AI, but the most prominent are:\n",
      "\n",
      "* **Large Language Models (LLMs):** These are the workhorses of many generative AI systems. Examples include:\n",
      "    * **GPT (Generative Pre-trained Transformer) series (e.g., ChatGPT, GPT-4):** Excellent at generating text, translating languages, writing different kinds of creative content, and answering your questions in a comprehensive manner.\n",
      "    * **LLaMA & Llama 2:** Open-source LLMs, making them more accessible for research and development.\n",
      "* **Generative Adversarial Networks (GANs):** These are used for creating realistic images, videos, and audio. They involve two neural networks ‚Äì a \"generator\" and a \"discriminator\" ‚Äì that compete against each other. The generator creates content, and the discriminator tries to tell the difference between the generated content and real content. This competition helps the generator produce more convincing results.\n",
      "* **Diffusion Models:** A more recent and powerful technique. These models start with random noise and gradually refine it into a coherent image or audio. They‚Äôve become incredibly popular for creating high-quality images.\n",
      "\n",
      "**3. How Does It Work (Simplified)?**\n",
      "\n",
      "Essentially, generative AI models are trained on huge datasets.  They learn the patterns and relationships within that data.  Then, when given a prompt (input), they use that knowledge to *predict* the next most likely element in the sequence.  \n",
      "\n",
      "* **Text:** The model learns the structure of language and can then generate new text that follows those patterns.\n",
      "* **Images:** The model learns what images look like based on the data it was trained on. It then uses this knowledge to create entirely new images.\n",
      "* **Audio:** Similar to images, it learns the patterns of audio and can generate new sounds.\n",
      "* **Video:**  It builds on the knowledge of images to create a sequence of frames.\n",
      "\n",
      "**4.  Examples of Generative AI in Action**\n",
      "\n",
      "* **ChatGPT & Other LLMs:**  Writing essays, code, answering questions, creating stories, translating languages.\n",
      "* **Image Generators (DALL-E, Midjourney, Stable Diffusion):** Creating images from text prompts (e.g., \"a cat wearing a hat in the style of Van Gogh\").\n",
      "* **Music Generators:** Creating original music pieces in different styles.\n",
      "* **Video Generators:** Creating short video clips from text descriptions or images.\n",
      "* **Code Generation:** Tools like GitHub Copilot can generate code snippets or even complete programs.\n",
      "\n",
      "\n",
      "**5.  Key Capabilities & Applications**\n",
      "\n",
      "* **Content Creation:** Writing articles, blog posts, marketing copy, scripts, poems, etc.\n",
      "* **Design:** Generating logos, website layouts, product mockups.\n",
      "* **Entertainment:** Creating games, special effects, and personalized content.\n",
      "* **Drug Discovery:** Generating potential drug candidates.\n",
      "* **Art & Design:**  Pushing creative boundaries and creating novel artistic styles.\n",
      "* **Personalization:** Tailoring content and experiences to individual users.\n",
      "\n",
      "\n",
      "**6. Important Considerations**\n",
      "\n",
      "* **Bias:** Generative AI models can inherit biases present in their training data, which can lead to unfair or discriminatory outputs.\n",
      "* **Copyright:**  The legal status of AI-generated content is still being debated.\n",
      "* **Ethical Concerns:** Deepfakes (realistic fake videos) and misuse of generative AI pose risks.\n",
      "\n",
      "**Resources for Further Learning:**\n",
      "\n",
      "* **OpenAI:** [https://openai.com/](https://openai.com/)\n",
      "* **Google AI:** [https://ai.googleblog.com/](https://ai.googleblog.com/)\n",
      "* **DeepLearning.AI:** [https://www.deeplearning.ai/](https://www.deeplearning.ai/)\n",
      "* **W3C Generative AI:** [https://generativeai.w3.org/](https://generativeai.w3.org/)\n",
      "\n",
      "---\n",
      "\n",
      "**To help me tailor my explanation even further, could you tell me:**\n",
      "\n",
      "*   **What specifically are you interested in learning about Generative AI?** (e.g., its impact on a specific industry, a particular technology, its ethical implications)?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Use your locally installed Ollama model\n",
    "llm = Ollama(model=\"gemma3:1b\")  # or \"llama2\", \"gemma\", etc. depending on what you pulled in Ollama\n",
    "\n",
    "# Test query\n",
    "response = llm.invoke(\"What is Generative AI?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4219789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you?\n",
      "I'm going to have a little break. It's only one week but I'll be back in two weeks and then we can talk about it again after that.\"\n"
     ]
    }
   ],
   "source": [
    "result = pipe(\"How are you?\")\n",
    "print(result[0].get('generated_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1767f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatprompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89c13207",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert AI Engineer\"),\n",
    "    (\"user\",\"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75b32db5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"ChatPromptValue\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# chain\u001b[39;00m\n\u001b[32m      2\u001b[39m chain = template | pipe\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m responce = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is LangChain?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(responce)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3049\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3048\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3049\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4781\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4767\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4768\u001b[39m \n\u001b[32m   4769\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4778\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4782\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4783\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4785\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4786\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4787\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4788\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1938\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1934\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1935\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1936\u001b[39m         output = cast(\n\u001b[32m   1937\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1946\u001b[39m         )\n\u001b[32m   1947\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1948\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4639\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4637\u001b[39m                 output = chunk\n\u001b[32m   4638\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4639\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4640\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4642\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:321\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    320\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1458\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1451\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1452\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1455\u001b[39m         )\n\u001b[32m   1456\u001b[39m     )\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1464\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1465\u001b[39m     model_outputs = \u001b[38;5;28mself\u001b[39m.forward(model_inputs, **forward_params)\n\u001b[32m   1466\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:359\u001b[39m, in \u001b[36mTextGenerationPipeline.preprocess\u001b[39m\u001b[34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[39m\n\u001b[32m    350\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer.apply_chat_template(\n\u001b[32m    351\u001b[39m         prompt_text.messages,\n\u001b[32m    352\u001b[39m         add_generation_prompt=\u001b[38;5;129;01mnot\u001b[39;00m continue_final_message,\n\u001b[32m   (...)\u001b[39m\u001b[32m    356\u001b[39m         **tokenizer_kwargs,\n\u001b[32m    357\u001b[39m     )\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_text\u001b[49m, return_tensors=\u001b[38;5;28mself\u001b[39m.framework, **tokenizer_kwargs)\n\u001b[32m    361\u001b[39m inputs[\u001b[33m\"\u001b[39m\u001b[33mprompt_text\u001b[39m\u001b[33m\"\u001b[39m] = prompt_text\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle_long_generation == \u001b[33m\"\u001b[39m\u001b[33mhole\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: can only concatenate str (not \"ChatPromptValue\") to str"
     ]
    }
   ],
   "source": [
    "# chain\n",
    "chain = template | pipe\n",
    "\n",
    "responce = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b63c9dc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'responce' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[43mresponce\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'responce' is not defined"
     ]
    }
   ],
   "source": [
    "type(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5008ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stroutput parser\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3d974e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"ChatPromptValue\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m output = StrOutputParser()\n\u001b[32m      2\u001b[39m chain = template | pipe | output\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m responce = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is LangChain?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(responce)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3049\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3048\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3049\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4781\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4767\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4768\u001b[39m \n\u001b[32m   4769\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4778\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4781\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4782\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4783\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4784\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4785\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4786\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4787\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4788\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1938\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1934\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1935\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1936\u001b[39m         output = cast(\n\u001b[32m   1937\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1938\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1939\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1946\u001b[39m         )\n\u001b[32m   1947\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1948\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4639\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4637\u001b[39m                 output = chunk\n\u001b[32m   4638\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4639\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4640\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4641\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4642\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    428\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:321\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    320\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1458\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1451\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1452\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1455\u001b[39m         )\n\u001b[32m   1456\u001b[39m     )\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1464\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1463\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1465\u001b[39m     model_outputs = \u001b[38;5;28mself\u001b[39m.forward(model_inputs, **forward_params)\n\u001b[32m   1466\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kami5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:359\u001b[39m, in \u001b[36mTextGenerationPipeline.preprocess\u001b[39m\u001b[34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[39m\n\u001b[32m    350\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer.apply_chat_template(\n\u001b[32m    351\u001b[39m         prompt_text.messages,\n\u001b[32m    352\u001b[39m         add_generation_prompt=\u001b[38;5;129;01mnot\u001b[39;00m continue_final_message,\n\u001b[32m   (...)\u001b[39m\u001b[32m    356\u001b[39m         **tokenizer_kwargs,\n\u001b[32m    357\u001b[39m     )\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_text\u001b[49m, return_tensors=\u001b[38;5;28mself\u001b[39m.framework, **tokenizer_kwargs)\n\u001b[32m    361\u001b[39m inputs[\u001b[33m\"\u001b[39m\u001b[33mprompt_text\u001b[39m\u001b[33m\"\u001b[39m] = prompt_text\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle_long_generation == \u001b[33m\"\u001b[39m\u001b[33mhole\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: can only concatenate str (not \"ChatPromptValue\") to str"
     ]
    }
   ],
   "source": [
    "output = StrOutputParser()\n",
    "chain = template | pipe | output\n",
    "responce = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "print(responce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed1426ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down Generative AI. It's a really hot topic right now, and it‚Äôs a bit complex, but here‚Äôs a breakdown in simple terms:\n",
      "\n",
      "**What is Generative AI?**\n",
      "\n",
      "At its core, Generative AI is a type of artificial intelligence that can *create* new content ‚Äì text, images, audio, video, code, and more ‚Äì based on the data it‚Äôs been trained on. Instead of just analyzing or classifying existing data, it *generates* new data that resembles the data it learned from.  Think of it like a student learning by example and then creating their own original work.\n",
      "\n",
      "**Here's a breakdown of the key components:**\n",
      "\n",
      "* **AI (Artificial Intelligence):** It's about creating machines that can perform tasks that typically require human intelligence ‚Äì like understanding, learning, problem-solving, and decision-making.\n",
      "* **Generative:** This means \"creating\" or \"producing.\"  The AI isn‚Äôt just retrieving information; it‚Äôs *generating* something new.\n",
      "* **Models:**  Generative AI relies on *models*. These are mathematical representations of patterns and relationships within the data.  Different models are built for different types of content.\n",
      "\n",
      "\n",
      "**Examples of Generative AI in Action:**\n",
      "\n",
      "* **Text Generation:**\n",
      "    * **ChatGPT & Bard:** These are examples of large language models that can write stories, poems, articles, summaries, translate languages, and even debug code.\n",
      "    * **Content Creation Tools:** Tools that help generate marketing copy, social media posts, or website content.\n",
      "* **Image Generation:**\n",
      "    * **DALL-E 2, Midjourney, Stable Diffusion:** These tools can create incredibly realistic or stylized images from text prompts. You can type in \"a cat wearing a hat in the style of Van Gogh\" and get a fantastic image.\n",
      "* **Music Generation:**\n",
      "    * **Amper Music, Jukebox (by OpenAI):** These tools can generate original music pieces in various genres.\n",
      "* **Video Generation:**\n",
      "    * **RunwayML, Synthesia:** These are emerging technologies that can create short videos from text or images.\n",
      "* **Code Generation:**\n",
      "    * **GitHub Copilot:**  An AI assistant that helps developers write code. \n",
      "\n",
      "\n",
      "**How Does it Work (Simplified)?**\n",
      "\n",
      "Most generative AI models use a technique called \"deep learning,\" which involves training neural networks on massive datasets. These networks learn to:\n",
      "\n",
      "1. **Understand Patterns:** Analyze the data to identify common patterns and relationships.\n",
      "2. **Generate New Data:** Based on those patterns, the model can start creating new data points that resemble the training data.\n",
      "3. **Refine and Improve:**  Through iterations, the model learns to generate content that's more consistent, accurate, and aesthetically pleasing.\n",
      "\n",
      "**Key Technologies Behind Generative AI:**\n",
      "\n",
      "* **Large Language Models (LLMs):** Like GPT, which excel at text generation.\n",
      "* **Generative Adversarial Networks (GANs):** Used to create realistic images or videos.  They involve two networks competing against each other.\n",
      "* **Diffusion Models:**  Becoming very popular for image generation ‚Äì they work by gradually adding noise to an image and then learning to reverse the process to create new images.\n",
      "\n",
      "**Important Note:** Generative AI is rapidly evolving.  It‚Äôs a very dynamic field, and the capabilities are continually increasing.\n",
      "\n",
      "**Resources for Learning More:**\n",
      "\n",
      "* **OpenAI:** [https://openai.com/](https://openai.com/)\n",
      "* **Google AI:** [https://ai.googleblog.com/](https://ai.googleblog.com/)\n",
      "* **DeepLearning.AI:** [https://www.deeplearning.ai/](https://www.deeplearning.ai/)\n",
      "\n",
      "---\n",
      "\n",
      "**To help me tailor my answer further, could you tell me:**\n",
      "\n",
      "*   What specifically are you interested in knowing about Generative AI? (e.g., are you curious about its applications, the technology behind it, or the ethical considerations?)\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"What is Generative AI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81208a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
